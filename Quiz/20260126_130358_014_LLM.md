# RAG理解度テスト - 問題014

## 問題情報
- **問題番号**: 014
- **カテゴリ**: LLMの知識
- **難易度**: 中級
- **作成日時**: 2026-01-26 13:03:58

## 問題文

**Q14. ローカルLLM（Ollama）とクラウドLLM（Gemini等）の違いを説明してください。このアプリでは、なぜOllamaを使用しているのでしょうか？また、コンテキストウィンドウとトークン制限について説明してください。**

---

## 回答欄

（ここに回答を記入してください）
未回答

---

## 評価欄

### 理解度評価
- [ ] レベル1（基礎）: 用語を説明できる、基本的な流れを理解している
- [ ] レベル2（応用）: 技術の仕組みを説明できる、手法の違いを理解している
- [ ] レベル3（実践）: 実装の詳細を説明できる、精度向上の方法を提案できる

### 評価コメント
**未回答のため評価できません。**

**解説：**
- **ローカルLLM（Ollama）とクラウドLLM（Gemini等）の違い**：
  - **ローカルLLM（Ollama）**：
    - 自分のPC上で実行されるため、データが外部に送信されない（プライバシー保護）
    - インターネット接続が不要
    - 無料で利用可能
    - ただし、PCの性能に依存し、クラウドLLMより性能が低い場合がある
  - **クラウドLLM（Gemini等）**：
    - クラウド上で実行されるため、高性能なモデルを利用できる
    - APIキーが必要で、使用量に応じて課金される
    - データがクラウドに送信される（プライバシーに注意）
    - インターネット接続が必要
- **このアプリでOllamaを使用している理由**：
  - 開発初期段階で、無料で検証できるため
  - プライバシーを保護できる（マニュアルデータが外部に送信されない）
  - 将来的にGeminiに切り替え可能な設計になっている（`app/llm/base.py`で抽象化）
- **コンテキストウィンドウとトークン制限**：
  - **コンテキストウィンドウ**：LLMが一度に処理できるテキストの最大長（例：4096トークン）
  - **トークン制限**：プロンプトと回答を合わせたトークン数の上限
  - このアプリでは、`app/core/settings.py`で`quiz_ollama_num_ctx`（コンテキスト上限）や`quiz_ollama_num_predict`（生成上限）を設定している

### 不足している知識
- ローカルLLMとクラウドLLMの違い
- このアプリでOllamaを選んだ理由
- コンテキストウィンドウとトークン制限の概念

### 改善点
- 上記の解説を参考に回答を記入してください

---

## 参考情報

このアプリの実装を確認する場合:
- `backend/app/llm/ollama.py` でOllamaの実装を確認
- `backend/app/llm/gemini.py` でGemini APIの実装を確認（将来対応）
- `docs/10_Gemini_API移行手順.md` を参照

# RAG理解度テスト - 問題010

## 問題情報
- **問題番号**: 010
- **カテゴリ**: EmbeddingとベクトルDB
- **難易度**: 中級
- **作成日時**: 2026-01-26 13:03:58

## 問題文

**Q10. Embeddingとは何ですか？なぜRAGにおいてEmbeddingが必要なのでしょうか？このアプリでは、どのようなEmbeddingモデルを使用していますか？また、E5モデルの特徴（query/passage prefix）について説明してください。**

---

## 回答欄

（ここに回答を記入してください）
エンベディングとはテキストをベクトル化する過程のことです。RAGでは検索を行う際にベクトル化された質問とドキュメント情報の近似値を測り結果として出すため、エンベディングが必要になります。
---

## 評価欄

### 理解度評価
- [x] レベル1（基礎）: 用語を説明できる、基本的な流れを理解している
- [ ] レベル2（応用）: 技術の仕組みを説明できる、手法の違いを理解している
- [ ] レベル3（実践）: 実装の詳細を説明できる、精度向上の方法を提案できる

### 評価コメント
**良い点：**
- Embeddingの基本的な役割（テキストをベクトル化）を理解している
- RAGでの必要性（検索のため）を認識している

**改善点：**
- **使用しているEmbeddingモデルが不足**。このアプリでは`intfloat/multilingual-e5-small`を使用（`app/rag/embedding.py`、`app/core/settings.py`で設定）
- **E5モデルの特徴（query/passage prefix）が不足**：
  - E5モデルは、質問には`"query: "`、文書には`"passage: "`というprefixを付ける必要がある
  - 実装では、`embed_query`関数で`"query: {query}"`、`embed_passages`関数で`"passage: {passage}"`という形式でprefixを付与している
  - このprefixを付けることで、質問と文書を区別し、より適切なEmbeddingを生成できる
- **なぜEmbeddingが必要か**の説明がやや簡潔。より詳しく：
  - テキストを数値ベクトルに変換することで、意味的な類似度を計算できる
  - 似た意味の文は似たベクトルになるため、「意味で検索」が可能になる

### 不足している知識
- 使用しているEmbeddingモデル（E5-small）
- E5モデルのquery/passage prefixの仕様
- Embeddingの次元数（384次元）

### 改善点
- 使用モデルとE5の特徴を説明できるとレベル2に到達
- なぜE5を選んだか、他のモデルとの違いを説明できるとレベル3

---

## 参考情報

このアプリの実装を確認する場合:
- `backend/app/rag/embedding.py` でEmbedding生成の実装を確認
- `backend/app/core/settings.py` で `embedding_model` パラメータを確認
